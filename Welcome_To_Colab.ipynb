{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apatils-fti/AI_ML_Automation/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "XRay Data Extraction Script - FINAL VERSION WITH HARDCODED KEYS\n",
        "Extracts all test executions and test cases from XRay/Jira\n",
        "Run this in Google Colab or locally\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# ============================================================================\n",
        "# HARDCODED API KEYS\n",
        "# ============================================================================\n",
        "\n",
        "XRAY_CLIENT_ID = \"34E4C280B65E4C63B3F103B86FBAD190\"\n",
        "XRAY_CLIENT_SECRET = \"60407aac541855f71996a7f48ee001cfecb5c744c51a1d2cd7f43104caef50cb\"\n",
        "\n",
        "# If you want to use Colab secrets instead, uncomment these lines:\n",
        "# from google.colab import userdata\n",
        "# XRAY_CLIENT_ID = userdata.get('XRAY_CLIENT_ID').strip()\n",
        "# XRAY_CLIENT_SECRET = userdata.get('XRAY_CLIENT_SECRET').strip()\n",
        "\n",
        "XRAY_GRAPHQL_URL = 'https://xray.cloud.getxray.app/api/v2/graphql'\n",
        "REQUEST_DELAY = 0.5\n",
        "\n",
        "def get_xray_token():\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    payload = {\"client_id\": XRAY_CLIENT_ID, \"client_secret\": XRAY_CLIENT_SECRET}\n",
        "    response = requests.post('https://xray.cloud.getxray.app/api/v2/authenticate',\n",
        "                            json=payload, headers=headers)\n",
        "    return response.json() if response.status_code == 200 else None\n",
        "\n",
        "def execute_graphql(graphql, token, max_retries=10):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {token}\"\n",
        "    }\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(XRAY_GRAPHQL_URL, json={\"query\": graphql}, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            elif response.status_code == 429:\n",
        "                time.sleep(60)\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            time.sleep(2)\n",
        "    return {\"error\": \"Max retries exceeded\"}\n",
        "\n",
        "print(\"üöÄ XRAY DATA EXTRACTION - GET ALL TEST EXECUTIONS & TEST CASES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load mapping\n",
        "mapping_df = pd.read_excel('App_to_Jira_Testing_Mapping_102925.xlsx', header=0)\n",
        "mapping_df.columns = mapping_df.iloc[0]\n",
        "mapping_df = mapping_df[1:].reset_index(drop=True)\n",
        "mapping_df = mapping_df.drop(columns=[mapping_df.columns[0]])\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(mapping_df)} apps from mapping\")\n",
        "\n",
        "# Create lookups for EVERY key in mapping\n",
        "key_to_info = {}\n",
        "\n",
        "for idx, row in mapping_df.iterrows():\n",
        "    app_name = row['App Name']\n",
        "    priority = row.get('Priority?')\n",
        "    status = row.get('Status')\n",
        "\n",
        "    for key_col, date_cols, phase_type in [\n",
        "        ('QA Issue Key', ('QA Initial Date', 'QA Due Date', 'QA Date'), 'QA'),\n",
        "        ('UAT Issue Key', ('UAT Intial Date', 'UAT Due Date', 'UAT Date'), 'UAT'),\n",
        "        ('IT Issue Key', ('IT Intial Date', 'IT Due Date', None), 'IT')\n",
        "    ]:\n",
        "        if pd.notna(row[key_col]):\n",
        "            key = str(row[key_col]).strip()\n",
        "            key_to_info[key] = {\n",
        "                'app_name': app_name,\n",
        "                'app_priority': priority,\n",
        "                'app_status': status,\n",
        "                'phase_type': phase_type,\n",
        "                'initial_date': row.get(date_cols[0]),\n",
        "                'due_date': row.get(date_cols[1]),\n",
        "                'completion_date': row.get(date_cols[2]) if date_cols[2] else None\n",
        "            }\n",
        "\n",
        "print(f\"‚úÖ Created lookups for {len(key_to_info)} keys from mapping\")\n",
        "\n",
        "xray_token = get_xray_token()\n",
        "projects = ['SBDS', 'SBIT', 'SBSR', 'SBSS']\n",
        "\n",
        "# Get ALL work items\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üìä GETTING ALL TEST EXECUTIONS\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "all_work_items = []\n",
        "\n",
        "for project in projects:\n",
        "    start = 0\n",
        "    limit = 100\n",
        "\n",
        "    while True:\n",
        "        graphql = f\"\"\"{{\n",
        "            getTestExecutions(jql: \"project = {project}\", start: {start}, limit: {limit}) {{\n",
        "                total\n",
        "                results {{\n",
        "                    issueId\n",
        "                    testEnvironments\n",
        "                    jira(fields: [\"key\", \"summary\", \"project\", \"status\", \"created\", \"updated\", \"parent\"])\n",
        "                }}\n",
        "            }}\n",
        "        }}\"\"\"\n",
        "\n",
        "        response = execute_graphql(graphql, xray_token)\n",
        "\n",
        "        if 'errors' in response or 'data' not in response:\n",
        "            break\n",
        "\n",
        "        results = response['data']['getTestExecutions']['results']\n",
        "        total = response['data']['getTestExecutions'].get('total', 0)\n",
        "\n",
        "        all_work_items.extend(results)\n",
        "\n",
        "        if len(results) < limit or start + len(results) >= total:\n",
        "            break\n",
        "\n",
        "        start += limit\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "\n",
        "    print(f\"üîç {project}: {len([w for w in all_work_items if w.get('jira', {}).get('project', {}).get('key') == project])} work items\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total: {len(all_work_items)} work items\")\n",
        "\n",
        "# Try matching by work item key OR parent key\n",
        "print(f\"\\nüìã Matching work items to mapping...\")\n",
        "matched_count = 0\n",
        "\n",
        "for work_item in all_work_items:\n",
        "    exec_jira = work_item.get('jira', {})\n",
        "    work_item_key = exec_jira.get('key')\n",
        "    parent = exec_jira.get('parent', {})\n",
        "    parent_key = parent.get('key') if parent else None\n",
        "\n",
        "    # Try matching work item key first\n",
        "    if work_item_key in key_to_info:\n",
        "        work_item['_matched_key'] = work_item_key\n",
        "        matched_count += 1\n",
        "    # Try matching parent key\n",
        "    elif parent_key and parent_key in key_to_info:\n",
        "        work_item['_matched_key'] = parent_key\n",
        "        matched_count += 1\n",
        "    else:\n",
        "        work_item['_matched_key'] = None\n",
        "\n",
        "print(f\"‚úÖ Matched {matched_count} work items to mapping\")\n",
        "\n",
        "# Process ALL work items (matched or not)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üìä EXTRACTING TEST CASES\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "detailed_data = []\n",
        "summary_data = []\n",
        "processed = 0\n",
        "\n",
        "for work_item in all_work_items:\n",
        "    exec_id = work_item['issueId']\n",
        "    exec_jira = work_item.get('jira', {})\n",
        "    work_item_key = exec_jira.get('key', 'Unknown')\n",
        "    work_item_summary = exec_jira.get('summary', 'Unknown')\n",
        "    matched_key = work_item.get('_matched_key')\n",
        "\n",
        "    if processed % 20 == 0:\n",
        "        print(f\"üìã Progress: {processed}/{len(all_work_items)} - {len(detailed_data)} test cases\")\n",
        "\n",
        "    try:\n",
        "        project_key = exec_jira.get('project', {}).get('key', 'Unknown')\n",
        "        environments = work_item.get('testEnvironments', [])\n",
        "        env_name = environments[0] if environments else 'Unknown'\n",
        "\n",
        "        work_item_status = exec_jira.get('status', {}).get('name', 'Unknown')\n",
        "        work_item_created = exec_jira.get('created')\n",
        "        work_item_updated = exec_jira.get('updated')\n",
        "\n",
        "        parent = exec_jira.get('parent', {})\n",
        "        parent_key = parent.get('key') if parent else None\n",
        "\n",
        "        # Get info from mapping if matched\n",
        "        if matched_key:\n",
        "            info = key_to_info[matched_key]\n",
        "            app_name = info['app_name']\n",
        "            app_priority = info['app_priority']\n",
        "            app_status = info['app_status']\n",
        "            phase_type = info['phase_type']\n",
        "            phase_initial_date = info['initial_date']\n",
        "            phase_due_date = info['due_date']\n",
        "            phase_completion_date = info['completion_date']\n",
        "        else:\n",
        "            app_name = 'Not in Mapping'\n",
        "            app_priority = None\n",
        "            app_status = None\n",
        "            phase_type = 'Unknown'\n",
        "            phase_initial_date = None\n",
        "            phase_due_date = None\n",
        "            phase_completion_date = None\n",
        "\n",
        "        # Get test cases\n",
        "        all_test_runs = []\n",
        "        test_start = 0\n",
        "        test_limit = 100\n",
        "\n",
        "        while True:\n",
        "            graphql_tests = f\"\"\"{{\n",
        "                getTestRuns(testExecIssueIds: [\"{exec_id}\"], start: {test_start}, limit: {test_limit}) {{\n",
        "                    total\n",
        "                    results {{\n",
        "                        status {{ name }}\n",
        "                        startedOn\n",
        "                        finishedOn\n",
        "                        test {{\n",
        "                            jira(fields: [\"key\", \"summary\", \"labels\", \"created\", \"creator\", \"status\", \"assignee\", \"priority\", \"components\", \"updated\"])\n",
        "                        }}\n",
        "                    }}\n",
        "                }}\n",
        "            }}\"\"\"\n",
        "\n",
        "            resp = execute_graphql(graphql_tests, xray_token)\n",
        "\n",
        "            if 'errors' in resp or 'data' not in resp or 'getTestRuns' not in resp['data']:\n",
        "                break\n",
        "\n",
        "            test_results = resp['data']['getTestRuns']['results']\n",
        "            test_total = resp['data']['getTestRuns'].get('total', 0)\n",
        "\n",
        "            all_test_runs.extend(test_results)\n",
        "\n",
        "            if len(test_results) < test_limit or len(all_test_runs) >= test_total:\n",
        "                break\n",
        "\n",
        "            test_start += test_limit\n",
        "            time.sleep(REQUEST_DELAY)\n",
        "\n",
        "        counts = {'PASSED': 0, 'FAILED': 0, 'BLOCKED': 0, 'EXECUTING': 0, 'TO DO': 0, 'PASS_THROUGH': 0}\n",
        "\n",
        "        for test in all_test_runs:\n",
        "            test_jira = test['test']['jira']\n",
        "            status = test['status']['name']\n",
        "\n",
        "            if status in counts:\n",
        "                counts[status] += 1\n",
        "\n",
        "            detailed_data.append({\n",
        "                'app_name': app_name,\n",
        "                'app_key': project_key,\n",
        "                'app_mapped_key': matched_key if matched_key else 'Not Mapped',\n",
        "                'app_priority': app_priority,\n",
        "                'app_status': app_status,\n",
        "                'test_phase_type': phase_type,\n",
        "                'phase_initial_date': phase_initial_date,\n",
        "                'phase_due_date': phase_due_date,\n",
        "                'phase_completion_date': phase_completion_date,\n",
        "                'work_item_id': exec_id,\n",
        "                'work_item_key': work_item_key,\n",
        "                'work_item_summary': work_item_summary,\n",
        "                'work_item_parent': parent_key if parent_key else 'No Parent',\n",
        "                'work_item_environment': env_name,\n",
        "                'work_item_status': work_item_status,\n",
        "                'work_item_created': work_item_created,\n",
        "                'work_item_updated': work_item_updated,\n",
        "                'test_key': test_jira['key'],\n",
        "                'test_summary': test_jira['summary'],\n",
        "                'test_labels': \", \".join(test_jira.get('labels', [])),\n",
        "                'test_created_date': test_jira.get('created', ''),\n",
        "                'test_updated_date': test_jira.get('updated', ''),\n",
        "                'test_created_by': test_jira.get('creator', {}).get('displayName', 'Unknown'),\n",
        "                'test_status': test_jira.get('status', {}).get('name', 'Unknown'),\n",
        "                'test_execution_status': status,\n",
        "                'test_assignee': test_jira.get('assignee', {}).get('displayName', 'Unassigned') if test_jira.get('assignee') else 'Unassigned',\n",
        "                'test_priority': test_jira.get('priority', {}).get('name', 'Unknown') if test_jira.get('priority') else 'Unknown',\n",
        "                'test_components': \", \".join([c['name'] for c in test_jira.get('components', [])]),\n",
        "                'execution_started_date': test.get('startedOn', ''),\n",
        "                'execution_finished_date': test.get('finishedOn', ''),\n",
        "            })\n",
        "\n",
        "        if len(all_test_runs) > 0:\n",
        "            summary_data.append({\n",
        "                'app_name': app_name,\n",
        "                'app_mapped_key': matched_key if matched_key else 'Not Mapped',\n",
        "                'app_priority': app_priority,\n",
        "                'test_phase_type': phase_type,\n",
        "                'phase_initial_date': phase_initial_date,\n",
        "                'phase_due_date': phase_due_date,\n",
        "                'phase_completion_date': phase_completion_date,\n",
        "                'work_item_key': work_item_key,\n",
        "                'work_item_parent': parent_key if parent_key else 'No Parent',\n",
        "                'work_item_environment': env_name,\n",
        "                'passed': counts['PASSED'],\n",
        "                'failed': counts['FAILED'],\n",
        "                'blocked': counts['BLOCKED'],\n",
        "                'executing': counts['EXECUTING'],\n",
        "                'todo': counts['TO DO'],\n",
        "                'pass_through': counts['PASS_THROUGH'],\n",
        "                'total_tests': len(all_test_runs)\n",
        "            })\n",
        "\n",
        "        processed += 1\n",
        "\n",
        "        if processed % 50 == 0:\n",
        "            xray_token = get_xray_token()\n",
        "\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "df_v1 = pd.DataFrame(summary_data)\n",
        "df_v2 = pd.DataFrame(detailed_data)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"‚úÖ COMPLETE!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"V1: {len(df_v1)} work items\")\n",
        "print(f\"V2: {len(df_v2)} test cases\")\n",
        "\n",
        "if not df_v2.empty:\n",
        "    print(f\"\\nüìä Mapped vs Not Mapped:\")\n",
        "    print(df_v2['app_mapped_key'].value_counts().head(20))\n",
        "\n",
        "    matched_df = df_v2[df_v2['app_mapped_key'] != 'Not Mapped']\n",
        "    print(f\"\\n‚úÖ {len(matched_df)} test cases matched to mapping\")\n",
        "    print(f\"‚ùå {len(df_v2) - len(matched_df)} test cases NOT in mapping\")\n",
        "\n",
        "filename_v1 = 'v1_summary_ALL_WORK_ITEMS.csv'\n",
        "filename_v2 = 'v2_detailed_ALL_WORK_ITEMS.csv'\n",
        "\n",
        "df_v1.to_csv(filename_v1, index=False)\n",
        "df_v2.to_csv(filename_v2, index=False)\n",
        "\n",
        "print(f\"\\nüíæ Files:\")\n",
        "print(f\"   {filename_v1}\")\n",
        "print(f\"   {filename_v2}\")\n",
        "\n",
        "# Try to download files (works in Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(filename_v1)\n",
        "    files.download(filename_v2)\n",
        "    print(\"\\nüì• Files downloaded!\")\n",
        "except:\n",
        "    print(\"\\nüí° Files saved to current directory\")\n",
        "\n",
        "print(\"\\nüéâ DATA EXTRACTED!\")\n",
        "print(\"   Filter in Power BI: app_mapped_key != 'Not Mapped' to see your apps\")"
      ],
      "metadata": {
        "id": "rhd8EAAzAoQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "XRay Data Extraction Script - FINAL VERSION WITH BETTER RATE LIMITING\n",
        "Uses JQL query to get only Test Executions with parents\n",
        "Structure: Parent App (SBSS-8) ‚Üí Work Item (SBSS-180) ‚Üí Tests (SBSS-245, etc.)\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================================================================\n",
        "# HARDCODED API KEYS\n",
        "# ============================================================================\n",
        "\n",
        "XRAY_CLIENT_ID = \"34E4C280B65E4C63B3F103B86FBAD190\"\n",
        "XRAY_CLIENT_SECRET = \"60407aac541855f71996a7f48ee001cfecb5c744c51a1d2cd7f43104caef50cb\"\n",
        "\n",
        "XRAY_GRAPHQL_URL = 'https://xray.cloud.getxray.app/api/v2/graphql'\n",
        "\n",
        "# Rate limiting settings\n",
        "BASE_DELAY = 1.0  # 1 second between requests\n",
        "RETRY_DELAY = 10  # 10 seconds on first retry\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "def get_xray_token():\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    payload = {\"client_id\": XRAY_CLIENT_ID, \"client_secret\": XRAY_CLIENT_SECRET}\n",
        "    response = requests.post('https://xray.cloud.getxray.app/api/v2/authenticate',\n",
        "                            json=payload, headers=headers)\n",
        "    return response.json() if response.status_code == 200 else None\n",
        "\n",
        "def execute_graphql(graphql, token, max_retries=MAX_RETRIES):\n",
        "    \"\"\"Execute GraphQL with exponential backoff and better rate limiting\"\"\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {token}\"\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(XRAY_GRAPHQL_URL, json={\"query\": graphql}, headers=headers, timeout=60)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            elif response.status_code == 429:\n",
        "                # Exponential backoff for rate limiting\n",
        "                wait_time = RETRY_DELAY * (2 ** attempt)  # 10s, 20s, 40s, 80s, 160s\n",
        "                print(f\"  ‚ö† Rate limited (attempt {attempt + 1}/{max_retries}), waiting {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"  ‚ö† HTTP {response.status_code}: {response.text[:200]}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(RETRY_DELAY)\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† Attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(RETRY_DELAY)\n",
        "\n",
        "    return {\"error\": \"Max retries exceeded\"}\n",
        "\n",
        "print(\"üöÄ XRAY DATA EXTRACTION - OPTIMIZED WITH RATE LIMITING\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Structure: Parent App ‚Üí Work Item (Test Execution) ‚Üí Tests\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "xray_token = get_xray_token()\n",
        "if not xray_token:\n",
        "    print(\"‚ùå Authentication failed!\")\n",
        "    exit(1)\n",
        "\n",
        "print(\"‚úÖ Authenticated\")\n",
        "\n",
        "# JQL Query - only get Test Executions with parents\n",
        "# Need to escape quotes in the JQL string for GraphQL\n",
        "JQL_QUERY = 'project in (SBB, SBDS, SBD, SBERP, SBIT, SBSR, SBSS) AND issuetype = \\\\\"Test Execution\\\\\" AND parent != null'\n",
        "\n",
        "print(f\"\\nüìã JQL Query: {JQL_QUERY}\")\n",
        "\n",
        "# Get ALL Test Executions using JQL\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"üìä STEP 1: GETTING TEST EXECUTIONS WITH PARENTS\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "all_work_items = []\n",
        "start = 0\n",
        "limit = 100\n",
        "page = 1\n",
        "\n",
        "while True:\n",
        "    print(f\"  Fetching page {page} (items {start + 1}-{start + limit})...\")\n",
        "\n",
        "    graphql = f\"\"\"{{\n",
        "        getTestExecutions(jql: \"{JQL_QUERY}\", start: {start}, limit: {limit}) {{\n",
        "            total\n",
        "            results {{\n",
        "                issueId\n",
        "                testEnvironments\n",
        "                jira(fields: [\n",
        "                    \"key\",\n",
        "                    \"summary\",\n",
        "                    \"project\",\n",
        "                    \"status\",\n",
        "                    \"created\",\n",
        "                    \"updated\",\n",
        "                    \"duedate\",\n",
        "                    \"resolutiondate\",\n",
        "                    \"parent\",\n",
        "                    \"assignee\",\n",
        "                    \"priority\",\n",
        "                    \"labels\",\n",
        "                    \"components\"\n",
        "                ])\n",
        "            }}\n",
        "        }}\n",
        "    }}\"\"\"\n",
        "\n",
        "    response = execute_graphql(graphql, xray_token)\n",
        "\n",
        "    if 'errors' in response:\n",
        "        print(f\"  ‚úó GraphQL Error: {response['errors']}\")\n",
        "        break\n",
        "\n",
        "    if 'data' not in response:\n",
        "        print(f\"  ‚úó No data in response\")\n",
        "        break\n",
        "\n",
        "    data = response['data']['getTestExecutions']\n",
        "    results = data.get('results', [])\n",
        "    total = data.get('total', 0)\n",
        "\n",
        "    if page == 1:\n",
        "        print(f\"  Total Test Executions found: {total}\")\n",
        "\n",
        "    all_work_items.extend(results)\n",
        "    print(f\"  ‚úì Fetched {len(results)} items (total so far: {len(all_work_items)})\")\n",
        "\n",
        "    if len(results) < limit or start + len(results) >= total:\n",
        "        break\n",
        "\n",
        "    start += limit\n",
        "    page += 1\n",
        "    time.sleep(BASE_DELAY)  # Rate limiting between pages\n",
        "\n",
        "print(f\"\\n‚úÖ Total Work Items: {len(all_work_items)}\")\n",
        "\n",
        "# Show breakdown by project\n",
        "if all_work_items:\n",
        "    projects_found = {}\n",
        "    for item in all_work_items:\n",
        "        proj = item.get('jira', {}).get('project', {}).get('key', 'Unknown')\n",
        "        projects_found[proj] = projects_found.get(proj, 0) + 1\n",
        "\n",
        "    print(f\"\\nüìä Breakdown by project:\")\n",
        "    for proj, count in sorted(projects_found.items()):\n",
        "        print(f\"   {proj}: {count} work items\")\n",
        "\n",
        "# Show sample parent structure\n",
        "if all_work_items:\n",
        "    print(f\"\\nüìã Sample parent app structure:\")\n",
        "    for i, item in enumerate(all_work_items[:5]):\n",
        "        exec_jira = item.get('jira', {})\n",
        "        work_key = exec_jira.get('key', 'Unknown')\n",
        "        parent = exec_jira.get('parent', {})\n",
        "        if parent:\n",
        "            parent_key = parent.get('key', 'N/A')\n",
        "            print(f\"   {work_key} ‚Üí Parent: {parent_key}\")\n",
        "\n",
        "# Extract ALL test cases from ALL work items\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"üìä STEP 2: EXTRACTING ALL TEST CASES\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"This will take time due to rate limiting (~{len(all_work_items)} work items)\")\n",
        "print(f\"Estimated time: {len(all_work_items) * 2 / 60:.1f} minutes\\n\")\n",
        "\n",
        "detailed_data = []\n",
        "summary_data = []\n",
        "processed = 0\n",
        "start_time = datetime.now()\n",
        "\n",
        "for work_item in all_work_items:\n",
        "    exec_id = work_item['issueId']\n",
        "    exec_jira = work_item.get('jira', {})\n",
        "\n",
        "    # Work Item details\n",
        "    work_item_key = exec_jira.get('key', 'Unknown')\n",
        "    work_item_summary = exec_jira.get('summary', 'Unknown')\n",
        "    project_key = exec_jira.get('project', {}).get('key', 'Unknown')\n",
        "\n",
        "    # Work Item dates\n",
        "    work_item_status = exec_jira.get('status', {}).get('name', 'Unknown')\n",
        "    work_item_created = exec_jira.get('created')\n",
        "    work_item_updated = exec_jira.get('updated')\n",
        "    work_item_due_date = exec_jira.get('duedate')\n",
        "    work_item_resolution_date = exec_jira.get('resolutiondate')\n",
        "\n",
        "    # Work Item other fields\n",
        "    work_item_assignee = exec_jira.get('assignee', {}).get('displayName', 'Unassigned') if exec_jira.get('assignee') else 'Unassigned'\n",
        "    work_item_priority = exec_jira.get('priority', {}).get('name', 'Unknown') if exec_jira.get('priority') else 'Unknown'\n",
        "    work_item_labels = ', '.join(exec_jira.get('labels', []))\n",
        "    work_item_components = ', '.join([c.get('name', '') for c in exec_jira.get('components', [])])\n",
        "\n",
        "    # Parent/App info (SBSS-8, etc.)\n",
        "    parent = exec_jira.get('parent', {})\n",
        "    if parent and isinstance(parent, dict):\n",
        "        parent_app_key = parent.get('key', 'No Parent')\n",
        "        parent_app_summary = (parent.get('summary') or\n",
        "                             parent.get('fields', {}).get('summary', '') if 'fields' in parent else '')\n",
        "    else:\n",
        "        parent_app_key = 'No Parent'\n",
        "        parent_app_summary = ''\n",
        "\n",
        "    # Environment\n",
        "    environments = work_item.get('testEnvironments', [])\n",
        "    env_name = environments[0] if environments else 'Unknown'\n",
        "\n",
        "    # Progress tracking\n",
        "    if processed % 10 == 0:\n",
        "        elapsed = (datetime.now() - start_time).total_seconds()\n",
        "        rate = processed / elapsed if elapsed > 0 else 0\n",
        "        remaining = len(all_work_items) - processed\n",
        "        eta_seconds = remaining / rate if rate > 0 else 0\n",
        "        eta_minutes = eta_seconds / 60\n",
        "\n",
        "        print(f\"üìã Progress: {processed}/{len(all_work_items)} ({processed/len(all_work_items)*100:.1f}%) - \"\n",
        "              f\"{len(detailed_data)} tests - ETA: {eta_minutes:.1f}min\")\n",
        "\n",
        "    try:\n",
        "        # Get all test cases for this work item\n",
        "        all_test_runs = []\n",
        "        test_start = 0\n",
        "        test_limit = 100\n",
        "        test_page = 1\n",
        "\n",
        "        while True:\n",
        "            graphql_tests = f\"\"\"{{\n",
        "                getTestRuns(testExecIssueIds: [\"{exec_id}\"], start: {test_start}, limit: {test_limit}) {{\n",
        "                    total\n",
        "                    results {{\n",
        "                        status {{ name color }}\n",
        "                        startedOn\n",
        "                        finishedOn\n",
        "                        test {{\n",
        "                            issueId\n",
        "                            jira(fields: [\n",
        "                                \"key\",\n",
        "                                \"summary\",\n",
        "                                \"labels\",\n",
        "                                \"created\",\n",
        "                                \"updated\",\n",
        "                                \"creator\",\n",
        "                                \"status\",\n",
        "                                \"assignee\",\n",
        "                                \"priority\",\n",
        "                                \"components\",\n",
        "                                \"duedate\",\n",
        "                                \"resolutiondate\"\n",
        "                            ])\n",
        "                        }}\n",
        "                    }}\n",
        "                }}\n",
        "            }}\"\"\"\n",
        "\n",
        "            resp = execute_graphql(graphql_tests, xray_token)\n",
        "\n",
        "            if 'errors' in resp or 'data' not in resp or 'getTestRuns' not in resp['data']:\n",
        "                break\n",
        "\n",
        "            test_results = resp['data']['getTestRuns']['results']\n",
        "            test_total = resp['data']['getTestRuns'].get('total', 0)\n",
        "\n",
        "            all_test_runs.extend(test_results)\n",
        "\n",
        "            if len(test_results) < test_limit or len(all_test_runs) >= test_total:\n",
        "                break\n",
        "\n",
        "            test_start += test_limit\n",
        "            test_page += 1\n",
        "            time.sleep(BASE_DELAY)  # Rate limiting between test pages\n",
        "\n",
        "        # Count test statuses\n",
        "        counts = {'PASSED': 0, 'FAILED': 0, 'BLOCKED': 0, 'EXECUTING': 0, 'TO DO': 0, 'PASS_THROUGH': 0, 'ABORTED': 0}\n",
        "\n",
        "        for test in all_test_runs:\n",
        "            test_jira = test['test']['jira']\n",
        "            status = test['status']['name']\n",
        "\n",
        "            if status in counts:\n",
        "                counts[status] += 1\n",
        "            else:\n",
        "                counts[status] = 1\n",
        "\n",
        "            # Add detailed test case row\n",
        "            detailed_data.append({\n",
        "                # Parent App level (SBSS-8, etc.)\n",
        "                'parent_app_key': parent_app_key,\n",
        "                'parent_app_summary': parent_app_summary,\n",
        "\n",
        "                # Project\n",
        "                'project': project_key,\n",
        "\n",
        "                # Work Item (Test Execution) level\n",
        "                'work_item_id': exec_id,\n",
        "                'work_item_key': work_item_key,\n",
        "                'work_item_summary': work_item_summary,\n",
        "                'work_item_status': work_item_status,\n",
        "                'work_item_created': work_item_created,\n",
        "                'work_item_updated': work_item_updated,\n",
        "                'work_item_due_date': work_item_due_date,\n",
        "                'work_item_resolution_date': work_item_resolution_date,\n",
        "                'work_item_assignee': work_item_assignee,\n",
        "                'work_item_priority': work_item_priority,\n",
        "                'work_item_labels': work_item_labels,\n",
        "                'work_item_components': work_item_components,\n",
        "                'work_item_environment': env_name,\n",
        "\n",
        "                # Test Case level\n",
        "                'test_id': test['test']['issueId'],\n",
        "                'test_key': test_jira['key'],\n",
        "                'test_summary': test_jira['summary'],\n",
        "                'test_status': test_jira.get('status', {}).get('name', 'Unknown'),\n",
        "                'test_execution_status': status,\n",
        "                'test_execution_color': test['status'].get('color', ''),\n",
        "                'test_created_date': test_jira.get('created', ''),\n",
        "                'test_updated_date': test_jira.get('updated', ''),\n",
        "                'test_due_date': test_jira.get('duedate', ''),\n",
        "                'test_resolution_date': test_jira.get('resolutiondate', ''),\n",
        "                'test_created_by': test_jira.get('creator', {}).get('displayName', 'Unknown'),\n",
        "                'test_assignee': test_jira.get('assignee', {}).get('displayName', 'Unassigned') if test_jira.get('assignee') else 'Unassigned',\n",
        "                'test_priority': test_jira.get('priority', {}).get('name', 'Unknown') if test_jira.get('priority') else 'Unknown',\n",
        "                'test_labels': \", \".join(test_jira.get('labels', [])),\n",
        "                'test_components': \", \".join([c['name'] for c in test_jira.get('components', [])]),\n",
        "\n",
        "                # Execution dates\n",
        "                'execution_started_date': test.get('startedOn', ''),\n",
        "                'execution_finished_date': test.get('finishedOn', ''),\n",
        "            })\n",
        "\n",
        "        # Add summary row for this work item\n",
        "        if len(all_test_runs) > 0:\n",
        "            summary_data.append({\n",
        "                'parent_app_key': parent_app_key,\n",
        "                'parent_app_summary': parent_app_summary,\n",
        "                'project': project_key,\n",
        "                'work_item_key': work_item_key,\n",
        "                'work_item_summary': work_item_summary,\n",
        "                'work_item_status': work_item_status,\n",
        "                'work_item_created': work_item_created,\n",
        "                'work_item_updated': work_item_updated,\n",
        "                'work_item_due_date': work_item_due_date,\n",
        "                'work_item_resolution_date': work_item_resolution_date,\n",
        "                'work_item_environment': env_name,\n",
        "                'passed': counts.get('PASSED', 0),\n",
        "                'failed': counts.get('FAILED', 0),\n",
        "                'blocked': counts.get('BLOCKED', 0),\n",
        "                'executing': counts.get('EXECUTING', 0),\n",
        "                'todo': counts.get('TO DO', 0),\n",
        "                'pass_through': counts.get('PASS_THROUGH', 0),\n",
        "                'aborted': counts.get('ABORTED', 0),\n",
        "                'total_tests': len(all_test_runs)\n",
        "            })\n",
        "\n",
        "        processed += 1\n",
        "\n",
        "        # Refresh token every 50 work items\n",
        "        if processed % 50 == 0:\n",
        "            print(f\"  üîÑ Refreshing authentication token...\")\n",
        "            xray_token = get_xray_token()\n",
        "\n",
        "        # Rate limiting between work items\n",
        "        time.sleep(BASE_DELAY)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö† Error processing {work_item_key}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Create dataframes\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "df_detailed = pd.DataFrame(detailed_data)\n",
        "\n",
        "total_time = (datetime.now() - start_time).total_seconds() / 60\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"‚úÖ EXTRACTION COMPLETE!\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"‚è±Ô∏è  Total time: {total_time:.1f} minutes\")\n",
        "print(f\"üìä Summary: {len(df_summary)} work items\")\n",
        "print(f\"üìä Detailed: {len(df_detailed)} test cases\")\n",
        "\n",
        "# Show breakdown\n",
        "if not df_summary.empty:\n",
        "    print(f\"\\nüìà Breakdown by Project:\")\n",
        "    print(df_summary.groupby('project')['work_item_key'].count())\n",
        "\n",
        "    print(f\"\\nüìà Top Parent Apps:\")\n",
        "    parent_counts = df_summary['parent_app_key'].value_counts().head(10)\n",
        "    print(parent_counts)\n",
        "\n",
        "    print(f\"\\nüìà Test Status Summary:\")\n",
        "    status_summary = df_summary[['passed', 'failed', 'blocked', 'todo']].sum()\n",
        "    print(status_summary)\n",
        "\n",
        "# Save files\n",
        "filename_summary = 'COMPLETE_summary_all_work_items.csv'\n",
        "filename_detailed = 'COMPLETE_detailed_all_tests.csv'\n",
        "\n",
        "df_summary.to_csv(filename_summary, index=False)\n",
        "df_detailed.to_csv(filename_detailed, index=False)\n",
        "\n",
        "print(f\"\\nüíæ Files saved:\")\n",
        "print(f\"   1. {filename_summary} ({len(df_summary)} rows, {len(df_summary.columns)} columns)\")\n",
        "print(f\"   2. {filename_detailed} ({len(df_detailed)} rows, {len(df_detailed.columns)} columns)\")\n",
        "\n",
        "# Show sample\n",
        "if not df_detailed.empty:\n",
        "    print(f\"\\nüìã Sample data:\")\n",
        "    sample = df_detailed[['parent_app_key', 'work_item_key', 'test_key', 'test_execution_status']].head(10)\n",
        "    print(sample.to_string(index=False))\n",
        "\n",
        "# Try to download (Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(filename_summary)\n",
        "    files.download(filename_detailed)\n",
        "    print(\"\\nüì• Files downloaded!\")\n",
        "except:\n",
        "    print(\"\\nüí° Files saved to current directory\")\n",
        "\n",
        "print(\"\\nüéâ COMPLETE!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fyP8Tz30H808",
        "outputId": "9b9ef79b-c7ce-406f-a5ca-0490094dbad8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ XRAY DATA EXTRACTION - OPTIMIZED WITH RATE LIMITING\n",
            "================================================================================\n",
            "Structure: Parent App ‚Üí Work Item (Test Execution) ‚Üí Tests\n",
            "================================================================================\n",
            "‚úÖ Authenticated\n",
            "\n",
            "üìã JQL Query: project in (SBB, SBDS, SBD, SBERP, SBIT, SBSR, SBSS) AND issuetype = \\\"Test Execution\\\" AND parent != null\n",
            "\n",
            "================================================================================\n",
            "üìä STEP 1: GETTING TEST EXECUTIONS WITH PARENTS\n",
            "================================================================================\n",
            "\n",
            "  Fetching page 1 (items 1-100)...\n",
            "  Total Test Executions found: 107\n",
            "  ‚úì Fetched 100 items (total so far: 100)\n",
            "  Fetching page 2 (items 101-200)...\n",
            "  ‚úì Fetched 7 items (total so far: 107)\n",
            "\n",
            "‚úÖ Total Work Items: 107\n",
            "\n",
            "üìä Breakdown by project:\n",
            "   SBB: 27 work items\n",
            "   SBD: 9 work items\n",
            "   SBIT: 14 work items\n",
            "   SBSR: 2 work items\n",
            "   SBSS: 55 work items\n",
            "\n",
            "üìã Sample parent app structure:\n",
            "   SBIT-84 ‚Üí Parent: SBIT-2\n",
            "   SBD-133 ‚Üí Parent: SBD-1\n",
            "   SBD-131 ‚Üí Parent: SBD-2\n",
            "   SBD-130 ‚Üí Parent: SBD-2\n",
            "   SBSS-179 ‚Üí Parent: SBSS-8\n",
            "\n",
            "================================================================================\n",
            "üìä STEP 2: EXTRACTING ALL TEST CASES\n",
            "================================================================================\n",
            "This will take time due to rate limiting (~107 work items)\n",
            "Estimated time: 3.6 minutes\n",
            "\n",
            "üìã Progress: 0/107 (0.0%) - 0 tests - ETA: 0.0min\n",
            "üìã Progress: 10/107 (9.3%) - 176 tests - ETA: 5.1min\n",
            "üìã Progress: 20/107 (18.7%) - 207 tests - ETA: 4.1min\n",
            "üìã Progress: 30/107 (28.0%) - 211 tests - ETA: 3.3min\n",
            "üìã Progress: 40/107 (37.4%) - 234 tests - ETA: 2.8min\n",
            "  üîÑ Refreshing authentication token...\n",
            "üìã Progress: 50/107 (46.7%) - 263 tests - ETA: 2.4min\n",
            "üìã Progress: 60/107 (56.1%) - 330 tests - ETA: 2.0min\n",
            "üìã Progress: 70/107 (65.4%) - 551 tests - ETA: 1.6min\n",
            "üìã Progress: 80/107 (74.8%) - 833 tests - ETA: 1.1min\n",
            "üìã Progress: 90/107 (84.1%) - 1031 tests - ETA: 0.7min\n",
            "  üîÑ Refreshing authentication token...\n",
            "üìã Progress: 100/107 (93.5%) - 1338 tests - ETA: 0.3min\n",
            "\n",
            "================================================================================\n",
            "‚úÖ EXTRACTION COMPLETE!\n",
            "================================================================================\n",
            "‚è±Ô∏è  Total time: 4.7 minutes\n",
            "üìä Summary: 55 work items\n",
            "üìä Detailed: 1368 test cases\n",
            "\n",
            "üìà Breakdown by Project:\n",
            "project\n",
            "SBB      9\n",
            "SBD      4\n",
            "SBIT     6\n",
            "SBSR     2\n",
            "SBSS    34\n",
            "Name: work_item_key, dtype: int64\n",
            "\n",
            "üìà Top Parent Apps:\n",
            "parent_app_key\n",
            "SBSS-8        4\n",
            "SBD-2         3\n",
            "QFPR-37276    2\n",
            "SBIT-3        2\n",
            "SBSS-23       2\n",
            "SBSS-24       2\n",
            "SBSS-17       2\n",
            "SBSS-21       2\n",
            "SBSS-16       2\n",
            "SBSS-25       2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìà Test Status Summary:\n",
            "passed        3\n",
            "failed        0\n",
            "blocked       0\n",
            "todo       1359\n",
            "dtype: int64\n",
            "\n",
            "üíæ Files saved:\n",
            "   1. COMPLETE_summary_all_work_items.csv (55 rows, 19 columns)\n",
            "   2. COMPLETE_detailed_all_tests.csv (1368 rows, 33 columns)\n",
            "\n",
            "üìã Sample data:\n",
            "parent_app_key work_item_key   test_key test_execution_status\n",
            "        SBIT-2       SBIT-84    SBIT-85                PASSED\n",
            "        SBIT-2       SBIT-84    SBIT-86                PASSED\n",
            "        SBIT-2       SBIT-84    SBIT-87             EXECUTING\n",
            "        SBIT-2       SBIT-84    SBIT-88             EXECUTING\n",
            "        SBIT-2       SBIT-84    SBIT-89             EXECUTING\n",
            "         SBD-2       SBD-130 QFFO-80390                 TO DO\n",
            "         SBD-2       SBD-130 QFFO-80451                 TO DO\n",
            "         SBD-2       SBD-130 QFFO-81150                 TO DO\n",
            "         SBD-2       SBD-130 QFFO-80274                 TO DO\n",
            "         SBD-2       SBD-130 QFFO-80927                 TO DO\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75534077-4787-40ce-b77b-abbd0e8e1949\", \"COMPLETE_summary_all_work_items.csv\", 13488)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_01b63988-ba6d-4928-8fa2-d9ddf3bb0f31\", \"COMPLETE_detailed_all_tests.csv\", 698977)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì• Files downloaded!\n",
            "\n",
            "üéâ COMPLETE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create Power BI IDs for linking tables\n",
        "Maps parent_app_key to Application Acronym from Clone & Convey Apps\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "print(\"üîó CREATING POWER BI IDs FOR TABLE LINKING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load the test data\n",
        "print(\"\\nüìÅ Loading test data...\")\n",
        "df_tests = pd.read_csv('COMPLETE_detailed_all_tests.csv')\n",
        "print(f\"   Loaded {len(df_tests)} test cases\")\n",
        "print(f\"   Unique parent apps: {df_tests['parent_app_key'].nunique()}\")\n",
        "\n",
        "# Load the apps reference data\n",
        "print(\"\\nüìÅ Loading apps reference data...\")\n",
        "df_apps = pd.read_excel('Clone_and_Convey_Apps.xlsx')\n",
        "# Remove empty rows\n",
        "df_apps = df_apps[df_apps['Application ID'].notna() | df_apps['Application Acronym'].notna()]\n",
        "print(f\"   Loaded {len(df_apps)} applications\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"IDENTIFIER CHOICE: Application Acronym\")\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÖ Application Acronym is unique, readable, and user-friendly\")\n",
        "print(\"‚úÖ Will be used as the primary ID for Power BI relationships\")\n",
        "\n",
        "# Create mapping from parent_app_summary to Application Acronym\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CREATING MAPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get unique parent apps from test data\n",
        "parent_apps = df_tests[['parent_app_key', 'parent_app_summary']].drop_duplicates()\n",
        "print(f\"\\nParent apps in test data: {len(parent_apps)}\")\n",
        "\n",
        "# Try to match by app name (fuzzy matching)\n",
        "def extract_app_name_from_summary(summary):\n",
        "    \"\"\"Extract the app name from parent_app_summary\"\"\"\n",
        "    if pd.isna(summary):\n",
        "        return None\n",
        "    # Pattern: \"Category Clone: APP NAME\" or \"Category: APP NAME\"\n",
        "    patterns = [\n",
        "        r'Clone:\\s*(.+?)(?:\\s*\\(|$)',  # Match \"Clone: APP NAME\"\n",
        "        r':\\s*(.+?)(?:\\s*\\(|$)',        # Match \": APP NAME\"\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, summary)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    return summary.strip()\n",
        "\n",
        "# Add extracted app name\n",
        "parent_apps['extracted_name'] = parent_apps['parent_app_summary'].apply(extract_app_name_from_summary)\n",
        "\n",
        "print(f\"\\nüìã Parent apps with extracted names:\")\n",
        "print(parent_apps[['parent_app_key', 'parent_app_summary', 'extracted_name']].head(10).to_string(index=False))\n",
        "\n",
        "# Create manual mapping for the ones we can identify\n",
        "manual_mapping = {\n",
        "    'Lambda Test': 'ACCELQ',  # Lambda Test is likely ACCELQ\n",
        "    'SFS': 'SFS',  # Dispatch Clone: SFS\n",
        "    'QF Mobile': 'QFMOBILE',  # QF Mobile\n",
        "    'Splunk': 'QF-SPLUNK',\n",
        "    'WALLARM': 'WALLARM',\n",
        "    # Add more as needed based on the data\n",
        "}\n",
        "\n",
        "# Try to match automatically\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MATCHING PARENT APPS TO APPLICATION ACRONYMS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a mapping dictionary\n",
        "app_id_mapping = {}\n",
        "\n",
        "for idx, row in parent_apps.iterrows():\n",
        "    parent_key = row['parent_app_key']\n",
        "    extracted = row['extracted_name']\n",
        "\n",
        "    # Try direct match in Application Name or Acronym\n",
        "    match = df_apps[\n",
        "        (df_apps['Application Name'].str.contains(extracted, case=False, na=False)) |\n",
        "        (df_apps['Application Acronym'].str.contains(extracted, case=False, na=False))\n",
        "    ]\n",
        "\n",
        "    if len(match) > 0:\n",
        "        acronym = match.iloc[0]['Application Acronym']\n",
        "        app_id_mapping[parent_key] = acronym\n",
        "    elif extracted in manual_mapping:\n",
        "        app_id_mapping[parent_key] = manual_mapping[extracted]\n",
        "    else:\n",
        "        # No match found - will use parent_key as fallback\n",
        "        app_id_mapping[parent_key] = f\"UNMAPPED_{parent_key}\"\n",
        "\n",
        "print(f\"\\n‚úÖ Created mappings for {len(app_id_mapping)} parent apps\")\n",
        "\n",
        "# Show the mappings\n",
        "print(f\"\\nüìã Mapping Results:\")\n",
        "mapped_count = sum(1 for v in app_id_mapping.values() if not v.startswith('UNMAPPED_'))\n",
        "unmapped_count = len(app_id_mapping) - mapped_count\n",
        "print(f\"   Mapped to acronyms: {mapped_count}\")\n",
        "print(f\"   Unmapped (need manual review): {unmapped_count}\")\n",
        "\n",
        "print(f\"\\nüìã Sample mappings:\")\n",
        "for i, (key, value) in enumerate(list(app_id_mapping.items())[:15]):\n",
        "    parent_summary = parent_apps[parent_apps['parent_app_key'] == key]['parent_app_summary'].iloc[0]\n",
        "    status = \"‚úÖ\" if not value.startswith('UNMAPPED_') else \"‚ö†Ô∏è\"\n",
        "    print(f\"   {status} {key} ‚Üí {value}\")\n",
        "    print(f\"      ({parent_summary})\")\n",
        "\n",
        "# Add the app_id to the test data\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ADDING APP_ID TO TEST DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_tests['app_id'] = df_tests['parent_app_key'].map(app_id_mapping)\n",
        "\n",
        "print(f\"\\n‚úÖ Added app_id column to test data\")\n",
        "print(f\"   Rows with app_id: {df_tests['app_id'].notna().sum()}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nüìã Sample of updated data:\")\n",
        "sample = df_tests[['parent_app_key', 'app_id', 'work_item_key', 'test_key']].head(10)\n",
        "print(sample.to_string(index=False))\n",
        "\n",
        "# Save the updated file\n",
        "output_file = 'COMPLETE_detailed_all_tests_WITH_APP_ID.csv'\n",
        "df_tests.to_csv(output_file, index=False)\n",
        "print(f\"\\nüíæ Saved: {output_file}\")\n",
        "\n",
        "# Create a reference table for Power BI\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CREATING APP REFERENCE TABLE FOR POWER BI\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "app_reference = []\n",
        "for parent_key, app_id in app_id_mapping.items():\n",
        "    parent_info = parent_apps[parent_apps['parent_app_key'] == parent_key].iloc[0]\n",
        "\n",
        "    # Try to find in apps reference\n",
        "    app_info = df_apps[df_apps['Application Acronym'] == app_id]\n",
        "\n",
        "    if len(app_info) > 0:\n",
        "        app_row = app_info.iloc[0]\n",
        "        app_reference.append({\n",
        "            'app_id': app_id,\n",
        "            'parent_app_key': parent_key,\n",
        "            'parent_app_summary': parent_info['parent_app_summary'],\n",
        "            'application_name': app_row['Application Name'],\n",
        "            'application_id_sysgen': app_row['Application ID'],\n",
        "            'starburst_applicable': app_row['Starburst Applicable'],\n",
        "            'final_disposition': app_row['Final Disposition'],\n",
        "            'functional_lane': app_row['Functional Lane'],\n",
        "        })\n",
        "    else:\n",
        "        app_reference.append({\n",
        "            'app_id': app_id,\n",
        "            'parent_app_key': parent_key,\n",
        "            'parent_app_summary': parent_info['parent_app_summary'],\n",
        "            'application_name': 'Not Found in Reference',\n",
        "            'application_id_sysgen': None,\n",
        "            'starburst_applicable': None,\n",
        "            'final_disposition': None,\n",
        "            'functional_lane': None,\n",
        "        })\n",
        "\n",
        "df_app_reference = pd.DataFrame(app_reference)\n",
        "\n",
        "reference_file = 'APP_REFERENCE_TABLE.csv'\n",
        "df_app_reference.to_csv(reference_file, index=False)\n",
        "print(f\"\\nüíæ Saved: {reference_file}\")\n",
        "\n",
        "print(f\"\\nüìä App Reference Table:\")\n",
        "print(f\"   Rows: {len(df_app_reference)}\")\n",
        "print(f\"   Columns: {', '.join(df_app_reference.columns)}\")\n",
        "\n",
        "# Show unmapped apps that need manual review\n",
        "unmapped = df_app_reference[df_app_reference['app_id'].str.startswith('UNMAPPED_', na=False)]\n",
        "if len(unmapped) > 0:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚ö†Ô∏è  UNMAPPED APPS - NEED MANUAL REVIEW\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\n{len(unmapped)} apps could not be automatically mapped:\")\n",
        "    print(unmapped[['app_id', 'parent_app_key', 'parent_app_summary']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüìÇ Output Files:\")\n",
        "print(f\"   1. {output_file}\")\n",
        "print(f\"      - Original test data with app_id column added\")\n",
        "print(f\"      - Use app_id to link to app reference table\")\n",
        "print(f\"\\n   2. {reference_file}\")\n",
        "print(f\"      - Master app reference table\")\n",
        "print(f\"      - Links app_id to parent_app_key and application details\")\n",
        "print(\"\\nüí° In Power BI:\")\n",
        "print(\"   - Link tables using 'app_id' field\")\n",
        "print(\"   - app_id is the primary key for relationships\")\n",
        "\n",
        "# Try to download (Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_file)\n",
        "    files.download(reference_file)\n",
        "    print(\"\\nüì• Files downloaded!\")\n",
        "except:\n",
        "    print(\"\\nüí° Files saved to current directory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "MCvRKEsQMyOb",
        "outputId": "4c82c16e-cd7e-4a11-ef61-9d7af55b2bc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó CREATING POWER BI IDs FOR TABLE LINKING\n",
            "================================================================================\n",
            "\n",
            "üìÅ Loading test data...\n",
            "   Loaded 1368 test cases\n",
            "   Unique parent apps: 32\n",
            "\n",
            "üìÅ Loading apps reference data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Clone_and_Convey_Apps.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1245914981.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load the apps reference data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìÅ Loading apps reference data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf_apps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Clone_and_Convey_Apps.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# Remove empty rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf_apps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_apps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_apps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Application ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mdf_apps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Application Acronym'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Clone_and_Convey_Apps.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Transform Jira Testing Data for Power BI\n",
        "- Fix all date columns\n",
        "- Extract test phase type (QA, UAT, IT)\n",
        "- Split columns by test phase\n",
        "- Map to reference data\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üîÑ TRANSFORMING JIRA TESTING DATA FOR POWER BI\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüìÅ Loading data...\")\n",
        "df_tests = pd.read_excel('Jira_Testing_Data.xlsx')\n",
        "print(f\"   Test data: {len(df_tests)} rows\")\n",
        "\n",
        "mapping_df = pd.read_excel('App to Jira Testing Mapping_102925.xlsx', header=1)\n",
        "# Clean mapping file - remove empty rows\n",
        "mapping_df = mapping_df[mapping_df['App Name'].notna()]\n",
        "print(f\"   Mapping data: {len(mapping_df)} apps\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: EXTRACT TEST PHASE TYPE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: EXTRACTING TEST PHASE TYPE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def extract_test_phase(summary, work_item_key):\n",
        "    \"\"\"Extract test phase from work_item_summary or work_item_key\"\"\"\n",
        "    if pd.isna(summary):\n",
        "        return 'Unknown'\n",
        "\n",
        "    summary_lower = str(summary).lower()\n",
        "\n",
        "    # Check for explicit mentions\n",
        "    if 'qa (' in summary_lower or '- qa ' in summary_lower or 'qa(' in summary_lower:\n",
        "        return 'QA'\n",
        "    elif 'uat (' in summary_lower or '- uat ' in summary_lower or 'uat(' in summary_lower:\n",
        "        return 'UAT'\n",
        "    elif 'it systems' in summary_lower or '- it ' in summary_lower:\n",
        "        return 'IT'\n",
        "\n",
        "    # Fallback to work_item_key pattern (not reliable but better than nothing)\n",
        "    return 'Unknown'\n",
        "\n",
        "df_tests['test_phase'] = df_tests.apply(\n",
        "    lambda row: extract_test_phase(row['work_item_summary'], row['work_item_key']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Test phase distribution:\")\n",
        "print(df_tests['test_phase'].value_counts())\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: MAP TO GET TEST PHASE FROM MAPPING FILE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: MAPPING TO GET DEFINITIVE TEST PHASE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create mapping dictionaries from mapping file\n",
        "qa_keys = {}\n",
        "uat_keys = {}\n",
        "it_keys = {}\n",
        "\n",
        "for idx, row in mapping_df.iterrows():\n",
        "    app_name = row['App Name']\n",
        "\n",
        "    if pd.notna(row['QA Issue Key']):\n",
        "        qa_keys[str(row['QA Issue Key']).strip()] = {\n",
        "            'app_name': app_name,\n",
        "            'test_phase': 'QA',\n",
        "            'initial_date': row.get('QA Initial Date'),\n",
        "            'due_date': row.get('QA Due Date'),\n",
        "            'completion_date': row.get('QA Date')\n",
        "        }\n",
        "\n",
        "    if pd.notna(row['UAT Issue Key']):\n",
        "        uat_keys[str(row['UAT Issue Key']).strip()] = {\n",
        "            'app_name': app_name,\n",
        "            'test_phase': 'UAT',\n",
        "            'initial_date': row.get('UAT Intial Date'),\n",
        "            'due_date': row.get('UAT Due Date'),\n",
        "            'completion_date': row.get('UAT Date')\n",
        "        }\n",
        "\n",
        "    if pd.notna(row['IT Issue Key']):\n",
        "        it_keys[str(row['IT Issue Key']).strip()] = {\n",
        "            'app_name': app_name,\n",
        "            'test_phase': 'IT',\n",
        "            'initial_date': row.get('IT Intial Date'),\n",
        "            'due_date': row.get('IT Due Date'),\n",
        "            'completion_date': None\n",
        "        }\n",
        "\n",
        "# Combine all mapping\n",
        "all_phase_keys = {**qa_keys, **uat_keys, **it_keys}\n",
        "\n",
        "print(f\"   QA keys: {len(qa_keys)}\")\n",
        "print(f\"   UAT keys: {len(uat_keys)}\")\n",
        "print(f\"   IT keys: {len(it_keys)}\")\n",
        "\n",
        "# Map both parent_app_key AND work_item_key to get test phase and dates\n",
        "def map_phase_info(parent_key, work_item_key):\n",
        "    \"\"\"Get phase info from mapping - check both parent and work item keys\"\"\"\n",
        "    keys_to_check = []\n",
        "\n",
        "    if pd.notna(parent_key):\n",
        "        keys_to_check.append(str(parent_key).strip())\n",
        "    if pd.notna(work_item_key):\n",
        "        keys_to_check.append(str(work_item_key).strip())\n",
        "\n",
        "    for key_str in keys_to_check:\n",
        "        if key_str in all_phase_keys:\n",
        "            info = all_phase_keys[key_str]\n",
        "            return pd.Series({\n",
        "                'mapped_test_phase': info['test_phase'],\n",
        "                'mapped_initial_date': info['initial_date'],\n",
        "                'mapped_due_date': info['due_date'],\n",
        "                'mapped_completion_date': info['completion_date']\n",
        "            })\n",
        "\n",
        "    return pd.Series({'mapped_test_phase': None, 'mapped_initial_date': None,\n",
        "                     'mapped_due_date': None, 'mapped_completion_date': None})\n",
        "\n",
        "# Apply mapping - check both parent_app_key and work_item_key\n",
        "phase_info = df_tests.apply(\n",
        "    lambda row: map_phase_info(row['parent_app_key'], row['work_item_key']),\n",
        "    axis=1\n",
        ")\n",
        "df_tests = pd.concat([df_tests, phase_info], axis=1)\n",
        "\n",
        "# Use mapped phase if available, otherwise use extracted\n",
        "df_tests['test_phase_final'] = df_tests['mapped_test_phase'].fillna(df_tests['test_phase'])\n",
        "\n",
        "print(f\"\\nüìä Final test phase distribution:\")\n",
        "print(df_tests['test_phase_final'].value_counts())\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: FIX ALL DATE COLUMNS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: CONVERTING ALL DATES TO PROPER DATE FORMAT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "date_columns = [\n",
        "    'work_item_created', 'work_item_updated', 'work_item_due_date', 'work_item_resolution_date',\n",
        "    'test_created_date', 'test_updated_date', 'test_due_date', 'test_resolution_date',\n",
        "    'execution_started_date', 'execution_finished_date',\n",
        "    'mapped_initial_date', 'mapped_due_date', 'mapped_completion_date'\n",
        "]\n",
        "\n",
        "def convert_to_date(value):\n",
        "    \"\"\"Convert various date formats to datetime (timezone-naive for Excel)\"\"\"\n",
        "    if pd.isna(value) or value == '' or value == 'NaT':\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Try parsing as datetime\n",
        "        dt = pd.to_datetime(value, errors='coerce')\n",
        "        if pd.notna(dt):\n",
        "            # Remove timezone info for Excel compatibility\n",
        "            if dt.tzinfo is not None:\n",
        "                dt = dt.tz_localize(None)\n",
        "            return dt\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "for col in date_columns:\n",
        "    if col in df_tests.columns:\n",
        "        print(f\"   Converting {col}...\")\n",
        "        df_tests[col] = df_tests[col].apply(convert_to_date)\n",
        "        non_null = df_tests[col].notna().sum()\n",
        "        print(f\"      ‚úì {non_null} non-null dates\")\n",
        "\n",
        "# Additional safety: strip timezone from all datetime columns\n",
        "print(\"\\n   Ensuring all dates are timezone-naive for Excel...\")\n",
        "for col in df_tests.columns:\n",
        "    if df_tests[col].dtype == 'datetime64[ns, UTC]' or 'datetime64[ns,' in str(df_tests[col].dtype):\n",
        "        df_tests[col] = df_tests[col].dt.tz_localize(None)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: CREATE PHASE-SPECIFIC COLUMNS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: CREATING PHASE-SPECIFIC COLUMNS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize phase-specific columns\n",
        "for phase in ['QA', 'UAT', 'IT']:\n",
        "    df_tests[f'{phase.lower()}_initial_date'] = None\n",
        "    df_tests[f'{phase.lower()}_due_date'] = None\n",
        "    df_tests[f'{phase.lower()}_completion_date'] = None\n",
        "    df_tests[f'{phase.lower()}_work_item_key'] = None\n",
        "    df_tests[f'{phase.lower()}_work_item_created'] = None\n",
        "    df_tests[f'{phase.lower()}_work_item_updated'] = None\n",
        "    df_tests[f'{phase.lower()}_execution_started'] = None\n",
        "    df_tests[f'{phase.lower()}_execution_finished'] = None\n",
        "\n",
        "# Populate phase-specific columns\n",
        "for idx, row in df_tests.iterrows():\n",
        "    phase = row['test_phase_final']\n",
        "\n",
        "    if pd.notna(phase) and phase in ['QA', 'UAT', 'IT']:\n",
        "        phase_lower = phase.lower()\n",
        "\n",
        "        # Dates from mapping\n",
        "        df_tests.at[idx, f'{phase_lower}_initial_date'] = row['mapped_initial_date']\n",
        "        df_tests.at[idx, f'{phase_lower}_due_date'] = row['mapped_due_date']\n",
        "        df_tests.at[idx, f'{phase_lower}_completion_date'] = row['mapped_completion_date']\n",
        "\n",
        "        # Work item info\n",
        "        df_tests.at[idx, f'{phase_lower}_work_item_key'] = row['work_item_key']\n",
        "        df_tests.at[idx, f'{phase_lower}_work_item_created'] = row['work_item_created']\n",
        "        df_tests.at[idx, f'{phase_lower}_work_item_updated'] = row['work_item_updated']\n",
        "\n",
        "        # Execution dates\n",
        "        df_tests.at[idx, f'{phase_lower}_execution_started'] = row['execution_started_date']\n",
        "        df_tests.at[idx, f'{phase_lower}_execution_finished'] = row['execution_finished_date']\n",
        "\n",
        "print(\"\\n‚úÖ Created phase-specific columns:\")\n",
        "print(\"   QA: initial_date, due_date, completion_date, work_item_key, etc.\")\n",
        "print(\"   UAT: initial_date, due_date, completion_date, work_item_key, etc.\")\n",
        "print(\"   IT: initial_date, due_date, completion_date, work_item_key, etc.\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: REORDER AND SELECT FINAL COLUMNS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: ORGANIZING FINAL COLUMN STRUCTURE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "final_columns = [\n",
        "    # Identifiers\n",
        "    'Application Acronym',\n",
        "    'parent_app_key',\n",
        "    'parent_app_summary',\n",
        "    'project',\n",
        "    'test_phase_final',\n",
        "\n",
        "    # QA Phase\n",
        "    'qa_work_item_key',\n",
        "    'qa_initial_date',\n",
        "    'qa_due_date',\n",
        "    'qa_completion_date',\n",
        "    'qa_work_item_created',\n",
        "    'qa_work_item_updated',\n",
        "    'qa_execution_started',\n",
        "    'qa_execution_finished',\n",
        "\n",
        "    # UAT Phase\n",
        "    'uat_work_item_key',\n",
        "    'uat_initial_date',\n",
        "    'uat_due_date',\n",
        "    'uat_completion_date',\n",
        "    'uat_work_item_created',\n",
        "    'uat_work_item_updated',\n",
        "    'uat_execution_started',\n",
        "    'uat_execution_finished',\n",
        "\n",
        "    # IT Phase\n",
        "    'it_work_item_key',\n",
        "    'it_initial_date',\n",
        "    'it_due_date',\n",
        "    'it_completion_date',\n",
        "    'it_work_item_created',\n",
        "    'it_work_item_updated',\n",
        "    'it_execution_started',\n",
        "    'it_execution_finished',\n",
        "\n",
        "    # Work Item Details (current phase)\n",
        "    'work_item_id',\n",
        "    'work_item_key',\n",
        "    'work_item_summary',\n",
        "    'work_item_status',\n",
        "    'work_item_assignee',\n",
        "    'work_item_priority',\n",
        "    'work_item_environment',\n",
        "\n",
        "    # Test Case Details\n",
        "    'test_id',\n",
        "    'test_key',\n",
        "    'test_summary',\n",
        "    'test_status',\n",
        "    'test_execution_status',\n",
        "    'test_execution_color',\n",
        "    'test_assignee',\n",
        "    'test_priority',\n",
        "    'test_labels',\n",
        "    'test_components',\n",
        "]\n",
        "\n",
        "# Select only columns that exist\n",
        "final_columns_exist = [col for col in final_columns if col in df_tests.columns]\n",
        "df_final = df_tests[final_columns_exist].copy()\n",
        "\n",
        "print(f\"\\n‚úÖ Final dataset:\")\n",
        "print(f\"   Rows: {len(df_final)}\")\n",
        "print(f\"   Columns: {len(df_final.columns)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: DATA QUALITY SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA QUALITY SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüìä Test Phase Coverage:\")\n",
        "for phase in ['QA', 'UAT', 'IT']:\n",
        "    count = (df_final['test_phase_final'] == phase).sum()\n",
        "    pct = count / len(df_final) * 100\n",
        "    print(f\"   {phase}: {count} tests ({pct:.1f}%)\")\n",
        "\n",
        "unknown = (df_final['test_phase_final'] == 'Unknown').sum()\n",
        "print(f\"   Unknown: {unknown} tests ({unknown/len(df_final)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìä Date Field Population:\")\n",
        "for phase in ['QA', 'UAT', 'IT']:\n",
        "    phase_lower = phase.lower()\n",
        "    due_count = df_final[f'{phase_lower}_due_date'].notna().sum()\n",
        "    complete_count = df_final[f'{phase_lower}_completion_date'].notna().sum()\n",
        "    exec_count = df_final[f'{phase_lower}_execution_started'].notna().sum()\n",
        "    print(f\"   {phase}:\")\n",
        "    print(f\"      Due dates: {due_count}\")\n",
        "    print(f\"      Completion dates: {complete_count}\")\n",
        "    print(f\"      Execution dates: {exec_count}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: SAVE OUTPUT\n",
        "# ============================================================================\n",
        "\n",
        "output_file = 'Jira_Testing_Data_PowerBI_Ready.xlsx'\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAVING OUTPUT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save to Excel with proper date formatting\n",
        "with pd.ExcelWriter(output_file, engine='openpyxl', datetime_format='yyyy-mm-dd') as writer:\n",
        "    df_final.to_excel(writer, sheet_name='Testing Data', index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved: {output_file}\")\n",
        "\n",
        "# Create a data dictionary\n",
        "print(\"\\nüìã Creating data dictionary...\")\n",
        "\n",
        "def get_column_description(col_name):\n",
        "    \"\"\"Get description for each column\"\"\"\n",
        "    descriptions = {\n",
        "        'Application Acronym': 'Application identifier/acronym',\n",
        "        'parent_app_key': 'Parent app Jira key (e.g., SBSS-8)',\n",
        "        'parent_app_summary': 'Parent app name/description',\n",
        "        'project': 'Jira project key',\n",
        "        'test_phase_final': 'Test phase: QA, UAT, or IT',\n",
        "        'qa_work_item_key': 'QA work item (Test Execution) key',\n",
        "        'qa_initial_date': 'QA testing initial/start date',\n",
        "        'qa_due_date': 'QA testing target due date',\n",
        "        'qa_completion_date': 'QA testing actual completion date',\n",
        "        'qa_work_item_created': 'QA work item created date',\n",
        "        'qa_work_item_updated': 'QA work item last updated date',\n",
        "        'qa_execution_started': 'QA test execution start date',\n",
        "        'qa_execution_finished': 'QA test execution completion date',\n",
        "        'uat_work_item_key': 'UAT work item (Test Execution) key',\n",
        "        'uat_initial_date': 'UAT testing initial/start date',\n",
        "        'uat_due_date': 'UAT testing target due date',\n",
        "        'uat_completion_date': 'UAT testing actual completion date',\n",
        "        'uat_work_item_created': 'UAT work item created date',\n",
        "        'uat_work_item_updated': 'UAT work item last updated date',\n",
        "        'uat_execution_started': 'UAT test execution start date',\n",
        "        'uat_execution_finished': 'UAT test execution completion date',\n",
        "        'it_work_item_key': 'IT work item (Test Execution) key',\n",
        "        'it_initial_date': 'IT testing initial/start date',\n",
        "        'it_due_date': 'IT testing target due date',\n",
        "        'it_completion_date': 'IT testing actual completion date',\n",
        "        'it_work_item_created': 'IT work item created date',\n",
        "        'it_work_item_updated': 'IT work item last updated date',\n",
        "        'it_execution_started': 'IT test execution start date',\n",
        "        'it_execution_finished': 'IT test execution completion date',\n",
        "        'work_item_id': 'Work item internal ID',\n",
        "        'work_item_key': 'Current work item Jira key',\n",
        "        'work_item_summary': 'Work item description',\n",
        "        'work_item_status': 'Work item status (Open, Done, etc.)',\n",
        "        'work_item_assignee': 'Person assigned to work item',\n",
        "        'work_item_priority': 'Work item priority level',\n",
        "        'work_item_environment': 'Testing environment',\n",
        "        'test_id': 'Test case internal ID',\n",
        "        'test_key': 'Individual test case key',\n",
        "        'test_summary': 'Test case description',\n",
        "        'test_status': 'Test case status',\n",
        "        'test_execution_status': 'Test result: PASSED, FAILED, BLOCKED, etc.',\n",
        "        'test_execution_color': 'Color code for test status',\n",
        "        'test_assignee': 'Person assigned to test case',\n",
        "        'test_priority': 'Test case priority level',\n",
        "        'test_labels': 'Test case labels/tags',\n",
        "        'test_components': 'Test case components',\n",
        "    }\n",
        "    return descriptions.get(col_name, '')\n",
        "\n",
        "data_dict = []\n",
        "for col in df_final.columns:\n",
        "    dtype = str(df_final[col].dtype)\n",
        "    non_null = df_final[col].notna().sum()\n",
        "    null_pct = (len(df_final) - non_null) / len(df_final) * 100\n",
        "\n",
        "    data_dict.append({\n",
        "        'Column Name': col,\n",
        "        'Data Type': dtype,\n",
        "        'Non-Null Count': non_null,\n",
        "        'Null %': f\"{null_pct:.1f}%\",\n",
        "        'Description': get_column_description(col)\n",
        "    })\n",
        "\n",
        "df_dict = pd.DataFrame(data_dict)\n",
        "\n",
        "dict_file = 'Data_Dictionary.xlsx'\n",
        "df_dict.to_excel(dict_file, index=False)\n",
        "print(f\"‚úÖ Saved: {dict_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ TRANSFORMATION COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüìÇ Output Files:\")\n",
        "print(f\"   1. {output_file}\")\n",
        "print(f\"      - Ready for Power BI import\")\n",
        "print(f\"      - All dates properly formatted\")\n",
        "print(f\"      - Separate columns for QA, UAT, IT\")\n",
        "print(f\"\\n   2. {dict_file}\")\n",
        "print(f\"      - Data dictionary with column descriptions\")\n",
        "\n",
        "print(\"\\nüí° Next Steps:\")\n",
        "print(\"   1. Import into Power BI\")\n",
        "print(\"   2. Create date table for time intelligence\")\n",
        "print(\"   3. Build relationships using parent_app_key\")\n",
        "print(\"   4. Create measures for pass rates, completion %, etc.\")\n",
        "\n",
        "# Try to download (Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_file)\n",
        "    files.download(dict_file)\n",
        "    print(\"\\nüì• Files downloaded!\")\n",
        "except:\n",
        "    print(\"\\nüí° Files saved to current directory\")\n",
        "\n",
        "print(\"\\nüéâ DONE!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F4cwi-E4Xgjp",
        "outputId": "5d6f67e7-dc4e-4ee4-86bf-85aabfc8345b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ TRANSFORMING JIRA TESTING DATA FOR POWER BI\n",
            "================================================================================\n",
            "\n",
            "üìÅ Loading data...\n",
            "   Test data: 1368 rows\n",
            "   Mapping data: 52 apps\n",
            "\n",
            "================================================================================\n",
            "STEP 1: EXTRACTING TEST PHASE TYPE\n",
            "================================================================================\n",
            "\n",
            "üìä Test phase distribution:\n",
            "test_phase\n",
            "QA         645\n",
            "UAT        634\n",
            "Unknown     84\n",
            "IT           5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "STEP 2: MAPPING TO GET DEFINITIVE TEST PHASE\n",
            "================================================================================\n",
            "   QA keys: 52\n",
            "   UAT keys: 52\n",
            "   IT keys: 1\n",
            "\n",
            "üìä Final test phase distribution:\n",
            "test_phase_final\n",
            "QA         645\n",
            "UAT        634\n",
            "Unknown     84\n",
            "IT           5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "STEP 3: CONVERTING ALL DATES TO PROPER DATE FORMAT\n",
            "================================================================================\n",
            "   Converting work_item_created...\n",
            "      ‚úì 1368 non-null dates\n",
            "   Converting work_item_updated...\n",
            "      ‚úì 1368 non-null dates\n",
            "   Converting work_item_due_date...\n",
            "      ‚úì 1030 non-null dates\n",
            "   Converting work_item_resolution_date...\n",
            "      ‚úì 55 non-null dates\n",
            "   Converting test_created_date...\n",
            "      ‚úì 1368 non-null dates\n",
            "   Converting test_updated_date...\n",
            "      ‚úì 1368 non-null dates\n",
            "   Converting test_due_date...\n",
            "      ‚úì 8 non-null dates\n",
            "   Converting test_resolution_date...\n",
            "      ‚úì 161 non-null dates\n",
            "   Converting execution_started_date...\n",
            "      ‚úì 10 non-null dates\n",
            "   Converting execution_finished_date...\n",
            "      ‚úì 3 non-null dates\n",
            "   Converting mapped_initial_date...\n",
            "      ‚úì 652 non-null dates\n",
            "   Converting mapped_due_date...\n",
            "      ‚úì 488 non-null dates\n",
            "   Converting mapped_completion_date...\n",
            "      ‚úì 794 non-null dates\n",
            "\n",
            "   Ensuring all dates are timezone-naive for Excel...\n",
            "\n",
            "================================================================================\n",
            "STEP 4: CREATING PHASE-SPECIFIC COLUMNS\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Created phase-specific columns:\n",
            "   QA: initial_date, due_date, completion_date, work_item_key, etc.\n",
            "   UAT: initial_date, due_date, completion_date, work_item_key, etc.\n",
            "   IT: initial_date, due_date, completion_date, work_item_key, etc.\n",
            "\n",
            "================================================================================\n",
            "STEP 5: ORGANIZING FINAL COLUMN STRUCTURE\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Final dataset:\n",
            "   Rows: 1368\n",
            "   Columns: 46\n",
            "\n",
            "================================================================================\n",
            "DATA QUALITY SUMMARY\n",
            "================================================================================\n",
            "\n",
            "üìä Test Phase Coverage:\n",
            "   QA: 645 tests (47.1%)\n",
            "   UAT: 634 tests (46.3%)\n",
            "   IT: 5 tests (0.4%)\n",
            "   Unknown: 84 tests (6.1%)\n",
            "\n",
            "üìä Date Field Population:\n",
            "   QA:\n",
            "      Due dates: 274\n",
            "      Completion dates: 347\n",
            "      Execution dates: 5\n",
            "   UAT:\n",
            "      Due dates: 214\n",
            "      Completion dates: 447\n",
            "      Execution dates: 0\n",
            "   IT:\n",
            "      Due dates: 0\n",
            "      Completion dates: 0\n",
            "      Execution dates: 5\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Saved: Jira_Testing_Data_PowerBI_Ready.xlsx\n",
            "\n",
            "üìã Creating data dictionary...\n",
            "‚úÖ Saved: Data_Dictionary.xlsx\n",
            "\n",
            "================================================================================\n",
            "‚úÖ TRANSFORMATION COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "üìÇ Output Files:\n",
            "   1. Jira_Testing_Data_PowerBI_Ready.xlsx\n",
            "      - Ready for Power BI import\n",
            "      - All dates properly formatted\n",
            "      - Separate columns for QA, UAT, IT\n",
            "\n",
            "   2. Data_Dictionary.xlsx\n",
            "      - Data dictionary with column descriptions\n",
            "\n",
            "üí° Next Steps:\n",
            "   1. Import into Power BI\n",
            "   2. Create date table for time intelligence\n",
            "   3. Build relationships using parent_app_key\n",
            "   4. Create measures for pass rates, completion %, etc.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_34acb576-7980-4277-a458-0f164e940a99\", \"Jira_Testing_Data_PowerBI_Ready.xlsx\", 260987)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_99edf7d7-7447-4514-8fa7-899d95a06554\", \"Data_Dictionary.xlsx\", 6669)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì• Files downloaded!\n",
            "\n",
            "üéâ DONE!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}